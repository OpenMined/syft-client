{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections.abc import Callable, Iterable\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wildchat_dataset = load_dataset(\"allenai/WildChat\")[\"train\"]\n",
    "print(\"Wildchat schema:\")\n",
    "for k, v in wildchat_dataset[0].items():\n",
    "    print(f\"{k}: {type(v)}\")\n",
    "\n",
    "# filter only english rows\n",
    "wildchat_dataset = wildchat_dataset.filter(lambda row: row[\"language\"] == \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Split into 1000 mock logs, and use the rest for private logs\n",
    "MAX_PRIVATE_LOGS = None\n",
    "MAX_MOCK_LOGS = 1000\n",
    "\n",
    "all_indices = np.arange(len(wildchat_dataset))\n",
    "np.random.shuffle(all_indices)\n",
    "\n",
    "assert MAX_MOCK_LOGS <= len(\n",
    "    all_indices\n",
    "), f\"Dataset is too small for {MAX_MOCK_LOGS} mock logs\"\n",
    "\n",
    "mock_logs = wildchat_dataset.select(all_indices[:MAX_MOCK_LOGS])\n",
    "private_logs = wildchat_dataset.select(all_indices[MAX_MOCK_LOGS:])\n",
    "\n",
    "if MAX_PRIVATE_LOGS is not None:\n",
    "    private_logs = private_logs.select(range(MAX_PRIVATE_LOGS))\n",
    "\n",
    "print(f\"Mock logs: {len(mock_logs)}\")\n",
    "print(f\"Private logs: {len(private_logs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return obj.replace(\"\\x00\", \"\")\n",
    "    if isinstance(obj, uuid.UUID):\n",
    "        return str(obj)\n",
    "    if isinstance(obj, (datetime.date, datetime.datetime)):\n",
    "        return obj.isoformat()\n",
    "    if isinstance(obj, list):\n",
    "        return [clean_json(v) for v in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_json(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "\n",
    "def process_wildchat_log(log_data: dict, max_content_length: int = 4096) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Convert a row of wildchat data into a list of messages with metadata\n",
    "\n",
    "    Expected output format:\n",
    "    [\n",
    "        {\n",
    "            \"id\": \"1234\" # A message ID, should be unique across all messages\n",
    "            \"text\": \"Hello, how are you?\", # The log content\n",
    "            **metadata, # Any metadata you want to store with the message\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "    Args:\n",
    "        log_data (dict): A single row of wildchat data\n",
    "        max_content_length (int, optional): Maximum length of the \"text\" field, depends on the embedding model.\n",
    "            Defaults to 4096 (~= 1000 tokens).\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of messages with metadata\n",
    "    \"\"\"\n",
    "\n",
    "    # Common metadata shared by all messages in the log\n",
    "    log_id = log_data[\"conversation_id\"]\n",
    "    base_metadata = {\n",
    "        \"log_id\": log_id,\n",
    "        \"model\": log_data[\"model\"],\n",
    "        \"timestamp\": log_data[\"timestamp\"],\n",
    "        \"total_turns\": log_data[\"turn\"],\n",
    "        \"language\": log_data[\"language\"],\n",
    "    }\n",
    "\n",
    "    result = []\n",
    "    for message_idx, message in enumerate(log_data[\"conversation\"]):\n",
    "        content = message[\"content\"]\n",
    "        if len(content) > max_content_length:\n",
    "            content = content[:max_content_length]\n",
    "\n",
    "        message_metadata = {\n",
    "            **base_metadata,\n",
    "            \"role\": message[\"role\"],\n",
    "            \"language\": message.get(\"language\", base_metadata[\"language\"]),\n",
    "            \"message_idx\": message_idx,\n",
    "        }\n",
    "\n",
    "        # deterministic + unique ID for each message\n",
    "        doc_id = uuid.uuid5(uuid.NAMESPACE_URL, f\"{log_id}_{message_idx}\")\n",
    "\n",
    "        result.append(\n",
    "            {\n",
    "                \"id\": str(doc_id),\n",
    "                \"text\": content,\n",
    "                **message_metadata,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Clean result so it can be inserted into Postgres\n",
    "    result = clean_json(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    dataset: Iterable[dict],\n",
    "    size: int | None = None,\n",
    ") -> list[dict]:\n",
    "    results = []\n",
    "\n",
    "    for log_data in tqdm(dataset, desc=\"Processing logs\", total=size):\n",
    "        processed = process_wildchat_log(log_data)\n",
    "        results.extend(processed)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_logs_processed = preprocess_dataset(\n",
    "    private_logs, size=len(private_logs)\n",
    ")\n",
    "mock_logs_processed = preprocess_dataset(\n",
    "    mock_logs, size=len(mock_logs)\n",
    ")\n",
    "\n",
    "print(f\"Processed private logs: {len(private_logs_processed):,} messages\")\n",
    "print(f\"Processed mock logs: {len(mock_logs_processed):,} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_df = pd.json_normalize(private_logs_processed)\n",
    "mock_df = pd.json_normalize(mock_logs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_data_path = \"private_logs.parquet\"\n",
    "mock_data_path = \"mock_logs.parquet\"\n",
    "\n",
    "private_df.to_parquet(private_data_path, index=False)\n",
    "mock_df.to_parquet(mock_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check filesizes\n",
    "\n",
    "!ls -lh {private_data_path} {mock_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syft-client",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
